"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç MNIST
2. –û–±—É—á–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏
3. –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (loss, accuracy)
4. –í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É

–ó–∞–ø—É—Å–∫:
    python demo.py
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, Subset
import matplotlib.pyplot as plt
from typing import Dict, Callable, List, Tuple, Optional
import time
import os

# –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
from torchvision import datasets, transforms

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from optimizers.conv_sgd import ConvolutionalSGD
from optimizers.local_loss_sgd import LocalLossConvSGD
from models.test_networks import SimpleNet, DeepNet, IllConditionedNet


def load_mnist(
    data_dir: str = "./data",
    train_samples: Optional[int] = 5000,
    test_samples: Optional[int] = 1000,
    batch_size: int = 64
) -> Tuple[DataLoader, DataLoader, int, int]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ MNIST.
    
    Args:
        data_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        train_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (None = –≤—Å–µ)
        test_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ (None = –≤—Å–µ)
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        
    Returns:
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        input_dim: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞ (784 –¥–ª—è MNIST)
        num_classes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (10)
    """
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: –≤ —Ç–µ–Ω–∑–æ—Ä + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ MNIST...")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    train_dataset = datasets.MNIST(
        root=data_dir, 
        train=True, 
        download=True, 
        transform=transform
    )
    test_dataset = datasets.MNIST(
        root=data_dir, 
        train=False, 
        download=True, 
        transform=transform
    )
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    if train_samples is not None and train_samples < len(train_dataset):
        train_dataset = Subset(train_dataset, range(train_samples))
    if test_samples is not None and test_samples < len(test_dataset):
        test_dataset = Subset(test_dataset, range(test_samples))
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"‚úì MNIST –∑–∞–≥—Ä—É–∂–µ–Ω: {len(train_dataset)} train, {len(test_dataset)} test")
    
    return train_loader, test_loader, 784, 10


def generate_data(
    n_samples: int = 1000, 
    n_features: int = 20, 
    n_classes: int = 10,
    noise: float = 0.1
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (fallback).
    
    –°–æ–∑–¥–∞—ë–º –ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —à—É–º–∞.
    
    Args:
        n_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        n_features: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        n_classes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        noise: —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
        
    Returns:
        X: —Ç–µ–Ω–∑–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (n_samples, n_features)
        y: —Ç–µ–Ω–∑–æ—Ä –º–µ—Ç–æ–∫ (n_samples,)
    """
    X = torch.randn(n_samples, n_features)
    W = torch.randn(n_features, n_classes)
    logits = X @ W + noise * torch.randn(n_samples, n_classes)
    y = logits.argmax(dim=1)
    return X, y


def train_and_evaluate(
    model: nn.Module, 
    optimizer: torch.optim.Optimizer, 
    train_loader: DataLoader, 
    test_loader: DataLoader, 
    epochs: int = 30,
    verbose: bool = True,
    flatten_input: bool = False
) -> Dict[str, List[float]]:
    """
    –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏.
    
    Args:
        model: –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        optimizer: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        flatten_input: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–¥–ª—è MNIST)
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏—Å—Ç–æ—Ä–∏–µ–π: train_loss, test_loss, accuracy, time_per_epoch
    """
    criterion = nn.CrossEntropyLoss()
    history = {
        'train_loss': [], 
        'test_loss': [], 
        'accuracy': [],
        'time_per_epoch': []
    }
    
    for epoch in range(epochs):
        start_time = time.time()
        
        # === TRAIN ===
        model.train()
        epoch_loss = 0
        for batch in train_loader:
            if len(batch) == 2:
                X, y = batch
            else:
                X, y = batch[0], batch[1]
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–µ—Ç–µ–π
            if flatten_input and X.dim() > 2:
                X = X.view(X.size(0), -1)
            
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            loss.backward()
            
            # –î–ª—è –Ω–∞—à–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –ø–µ—Ä–µ–¥–∞—ë–º closure –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–¥—Ä–∞
            if isinstance(optimizer, (ConvolutionalSGD, LocalLossConvSGD)):
                def closure():
                    out = model(X)
                    return criterion(out, y)
                optimizer.step(closure)
            else:
                optimizer.step()
            
            epoch_loss += loss.item()
        
        history['train_loss'].append(epoch_loss / len(train_loader))
        
        # === EVALUATE ===
        model.eval()
        correct = 0
        total = 0
        test_loss = 0
        with torch.no_grad():
            for batch in test_loader:
                if len(batch) == 2:
                    X, y = batch
                else:
                    X, y = batch[0], batch[1]
                
                if flatten_input and X.dim() > 2:
                    X = X.view(X.size(0), -1)
                
                output = model(X)
                test_loss += criterion(output, y).item()
                predictions = output.argmax(dim=1)
                correct += (predictions == y).sum().item()
                total += len(y)
        
        history['test_loss'].append(test_loss / len(test_loader))
        history['accuracy'].append(100 * correct / total)
        history['time_per_epoch'].append(time.time() - start_time)
        
        if verbose and (epoch + 1) % 5 == 0:
            print(f"  Epoch {epoch+1:3d}: "
                  f"Loss={history['train_loss'][-1]:.4f}, "
                  f"Acc={history['accuracy'][-1]:.1f}%")
    
    return history


def run_comparison(
    model_class: type = SimpleNet,
    model_name: str = "SimpleNet",
    epochs: int = 30,
    seed: int = 42,
    use_mnist: bool = True,
    input_dim: int = 784,
    output_dim: int = 10
):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏.
    
    Args:
        model_class: –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        seed: random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        use_mnist: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MNIST –∏–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        input_dim: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞
        output_dim: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
    """
    print("="*60)
    print(f"  –°–†–ê–í–ù–ï–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–û–í: {model_name}")
    print("="*60)
    
    torch.manual_seed(seed)
    
    if use_mnist:
        train_loader, test_loader, input_dim, output_dim = load_mnist(
            train_samples=5000, 
            test_samples=1000
        )
        flatten_input = True
    else:
        # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (fallback)
        X_train, y_train = generate_data(2000, n_features=input_dim, n_classes=output_dim)
        X_test, y_test = generate_data(500, n_features=input_dim, n_classes=output_dim)
        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)
        test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64)
        flatten_input = False
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
    optimizers_config: Dict[str, Callable] = {
        'SGD': lambda p: torch.optim.SGD(p, lr=0.01, momentum=0.9),
        'Adam': lambda p: torch.optim.Adam(p, lr=0.001),
        'ConvSGD': lambda p: ConvolutionalSGD(
            p, lr=0.01, momentum=0.9, kernel_size=3, adaptive_kernel=True
        ),
        'LocalLossConvSGD': lambda p: LocalLossConvSGD(
            p, lr=0.01, momentum=0.9, kernel_size=3
        ),
    }
    
    results: Dict[str, Dict] = {}
    
    for name, opt_fn in optimizers_config.items():
        print(f"\n--- {name} ---")
        torch.manual_seed(seed)  # –û–¥–∏–Ω–∞–∫–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        
        # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏
        if model_class == SimpleNet:
            model = SimpleNet(input_dim=input_dim, hidden_dim=128, output_dim=output_dim)
        elif model_class == DeepNet:
            model = DeepNet(input_dim=input_dim, hidden_dim=64, output_dim=output_dim)
        elif model_class == IllConditionedNet:
            model = IllConditionedNet(input_dim=input_dim, output_dim=output_dim)
        else:
            model = model_class()
        
        optimizer = opt_fn(model.parameters())
        results[name] = train_and_evaluate(
            model, optimizer, train_loader, test_loader, epochs, 
            flatten_input=flatten_input
        )
    
    return results


def visualize_results(
    results: Dict[str, Dict], 
    title: str = "Optimizer Comparison",
    save_path: str = "optimizer_comparison.png"
):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
    
    Args:
        results: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ {optimizer_name: history}
        title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    for (name, hist), color in zip(results.items(), colors):
        axes[0].plot(hist['train_loss'], label=name, color=color, linewidth=2)
        axes[1].plot(hist['test_loss'], label=name, color=color, linewidth=2)
        axes[2].plot(hist['accuracy'], label=name, color=color, linewidth=2)
    
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss')
    axes[0].legend()
    axes[0].set_yscale('log')
    axes[0].grid(True, alpha=0.3)
    
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].set_title('Test Loss')
    axes[1].legend()
    axes[1].set_yscale('log')
    axes[1].grid(True, alpha=0.3)
    
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Accuracy (%)')
    axes[2].set_title('Test Accuracy')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.suptitle(title, fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\n‚úì –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")


def print_summary(results: Dict[str, Dict]):
    """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print("\n" + "="*70)
    print("  –ò–¢–û–ì–ò")
    print("="*70)
    print(f"{'–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä':<20} {'Final Loss':>12} {'Final Acc':>12} {'Avg Time/Ep':>15}")
    print("-"*70)
    
    for name, hist in results.items():
        final_loss = hist['train_loss'][-1]
        final_acc = hist['accuracy'][-1]
        avg_time = sum(hist['time_per_epoch']) / len(hist['time_per_epoch'])
        print(f"{name:<20} {final_loss:>12.4f} {final_acc:>11.1f}% {avg_time:>14.4f}s")
    
    print("-"*70)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    best_by_loss = min(results.items(), key=lambda x: x[1]['train_loss'][-1])
    best_by_acc = max(results.items(), key=lambda x: x[1]['accuracy'][-1])
    
    print(f"\nüèÜ –õ—É—á—à–∏–π –ø–æ Loss: {best_by_loss[0]} ({best_by_loss[1]['train_loss'][-1]:.4f})")
    print(f"üèÜ –õ—É—á—à–∏–π –ø–æ Accuracy: {best_by_acc[0]} ({best_by_acc[1]['accuracy'][-1]:.1f}%)")


def run_all_experiments():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –Ω–∞ MNIST"""
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: SimpleNet –Ω–∞ MNIST
    print("\n" + "="*70)
    print("  –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: SimpleNet –Ω–∞ MNIST")
    print("="*70)
    results_simple = run_comparison(SimpleNet, "SimpleNet (MNIST)", epochs=20, use_mnist=True)
    visualize_results(results_simple, "SimpleNet on MNIST", "simple_comparison.png")
    print_summary(results_simple)
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: DeepNet –Ω–∞ MNIST
    print("\n\n" + "="*70)
    print("  –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: DeepNet (6 —Å–ª–æ—ë–≤) –Ω–∞ MNIST")
    print("="*70)
    results_deep = run_comparison(DeepNet, "DeepNet (MNIST)", epochs=20, use_mnist=True)
    visualize_results(results_deep, "DeepNet on MNIST", "deep_comparison.png")
    print_summary(results_deep)
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: IllConditionedNet –Ω–∞ MNIST
    print("\n\n" + "="*70)
    print("  –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: IllConditionedNet –Ω–∞ MNIST")
    print("="*70)
    results_ill = run_comparison(IllConditionedNet, "IllConditionedNet (MNIST)", epochs=20, use_mnist=True)
    visualize_results(results_ill, "IllConditionedNet on MNIST", "ill_conditioned_comparison.png")
    print_summary(results_ill)
    
    return {
        'SimpleNet': results_simple,
        'DeepNet': results_deep,
        'IllConditionedNet': results_ill
    }


def demo_kernel_evolution():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç–≤–æ–ª—é—Ü–∏–∏ —è–¥—Ä–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
    print("\n" + "="*70)
    print("  –î–ï–ú–û: –≠–≤–æ–ª—é—Ü–∏—è —è–¥—Ä–∞ ConvolutionalSGD")
    print("="*70)
    
    torch.manual_seed(42)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º MNIST
    train_loader, _, input_dim, output_dim = load_mnist(train_samples=2000, test_samples=500)
    
    model = SimpleNet(input_dim=input_dim, hidden_dim=128, output_dim=output_dim)
    optimizer = ConvolutionalSGD(model.parameters(), lr=0.01, kernel_size=5, adaptive_kernel=True)
    criterion = nn.CrossEntropyLoss()
    
    print(f"\n–ù–∞—á–∞–ª—å–Ω–æ–µ —è–¥—Ä–æ: {optimizer.get_kernel().numpy()}")
    
    for epoch in range(10):
        for batch in train_loader:
            X, y = batch
            X = X.view(X.size(0), -1)  # Flatten –¥–ª—è MNIST
            
            optimizer.zero_grad()
            loss = criterion(model(X), y)
            loss.backward()
            
            def closure():
                return criterion(model(X), y)
            optimizer.step(closure)
    
    print(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ —è–¥—Ä–æ: {optimizer.get_kernel().numpy()}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —è–¥—Ä–∞
    kernel_history = optimizer.get_kernel_history()
    if len(kernel_history) > 1:
        fig, ax = plt.subplots(figsize=(10, 4))
        kernel_matrix = torch.stack(kernel_history).numpy()
        
        for i in range(kernel_matrix.shape[1]):
            ax.plot(kernel_matrix[:, i], label=f'K[{i}]', linewidth=2)
        
        ax.set_xlabel('Iteration')
        ax.set_ylabel('Kernel Value')
        ax.set_title('Kernel Evolution During Training')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('kernel_evolution.png', dpi=150)
        print(f"\n‚úì –ì—Ä–∞—Ñ–∏–∫ —ç–≤–æ–ª—é—Ü–∏–∏ —è–¥—Ä–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: kernel_evolution.png")


if __name__ == "__main__":
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë          –°–í–ï–†–¢–û–ß–ù–´–ô –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† –î–õ–Ø –ù–ï–ô–†–û–°–ï–¢–ï–ô               ‚ïë
    ‚ïë                                                               ‚ïë
    ‚ïë   –î–∞—Ç–∞—Å–µ—Ç: MNIST (—Ä—É–∫–æ–ø–∏—Å–Ω—ã–µ —Ü–∏—Ñ—Ä—ã)                          ‚ïë
    ‚ïë   –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: SGD, Adam, ConvSGD, LocalLossConvSGD            ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    all_results = run_all_experiments()
    
    # –î–µ–º–æ —ç–≤–æ–ª—é—Ü–∏–∏ —è–¥—Ä–∞
    demo_kernel_evolution()
    
    print("\n\n" + "="*70)
    print("  –ì–û–¢–û–í–û! –°–æ–∑–¥–∞–Ω—ã –≥—Ä–∞—Ñ–∏–∫–∏:")
    print("  - simple_comparison.png")
    print("  - deep_comparison.png")
    print("  - ill_conditioned_comparison.png")
    print("  - kernel_evolution.png")
    print("="*70)
